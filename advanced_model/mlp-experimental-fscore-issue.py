# -*- coding: utf-8 -*-
"""2021-07-23-mlp-experimental.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CVyq2KY-RW2wuXVW7u19azmmM8DQ2AnA
"""


from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
import numpy as np
from itertools import combinations

# ===================================
'''(Jiwon): here change the data path if in need'''
# ===================================
data_root = Path('../datasets/emotions/isear/rule-driven')
# see all files in target dataroot
list(data_root.iterdir())
train_path = data_root/'train_val_occ_rule.csv'
test_path = data_root/'test_occ_rule.csv'
# you don't need to make this again if you have
#df1, df2 = pd.read_csv(data_root/'train_occ_rule.csv', index_col = 0), pd.read_csv(data_root/'valid_occ_rule.csv', index_col = 0)
#df1.append(df2, ignore_index=True).to_csv(train_path)
#del df1, df2
# ===================================



# ===================================
'''PART 1 - Loading data
(Jiwon): pandas has rather cumbersome especially when you need to combine several columns, so that just read it as basic I/O
'''
# ===================================

def get_data(data_path):
    label_ls, text_ls = [], []
    with data_path.open() as f:
        for line_idx, line in enumerate(f):
            if line_idx == 0:
                pass
            else:
                one_line = line.strip('\n\'\"\"').split(',', maxsplit=2)
                label_ls.append(one_line[1])
                #print(one_line)

                input = one_line[2].rsplit(',', maxsplit=3)
                text_ls.append(' \t '.join(input).strip())
                #tmp = ' \t '.join(input).strip()
                #text_ls.append(tmp.strip('\n\"\"\"').split('\t'))
                #print(text_ls)
                #print("this is a mark")
                #feat = list(list(combinations(input, i + 1)) for i, _ in enumerate(input))
                #print(feat)
    #print(text_ls)
    return label_ls, text_ls

# load plain data
y_train_raw, x_train_raw = get_data(train_path)
y_test_raw, x_test_raw = get_data(test_path)
#print(x_train_raw)
# convert to tfidf, for text data
# since default transform type is csr.sparse, need to be converted np.array
# TODO: there might be config arg in fit_transform or TfidfVectorizer, find out

vectorizer = TfidfVectorizer()
x_train = vectorizer.fit_transform(x_train_raw).toarray()
x_test = vectorizer.transform(x_test_raw).toarray()
#print(x_train)

# make target/label look-up table && recast to np array of int
label_to_idx = {label:idx for idx, label in enumerate(sorted(set(y_train_raw)))}
idx_to_label = {v:k for k, v in label_to_idx.items()}
y_train = np.array(list(map(lambda x: label_to_idx[x], y_train_raw)))
y_test = np.array(list(map(lambda x: label_to_idx[x], y_test_raw)))

# x_train.shape, y_train.shape

# ===================================
'''PART 2 - Set model and its configuration
(Jiwon): here comes the model
'''
# ===================================


import tensorflow as tf
import numpy as np
from tensorflow.keras import Model, layers, metrics, losses
from tensorflow_addons.metrics import F1Score #install 'tensorflow-addons' if you need
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras.utils import plot_model
#  TODO: I can make _package.py or __init__ and set __all__ = [ everything you need ]

def get_layers():
    input = layers.Input(shape=x_train.shape[1], name="input")
    dense_layer_1 = layers.Dense(768, activation="relu", name="dense_layer_1")(input)
    dense_layer_2 = layers.Dense(128, activation='relu', name="dense_layer_2")(dense_layer_1)
    dense_layer_3 = layers.Dense(32, activation='relu', name="dense_layer_3")(dense_layer_2)
    dense_layer_4 = layers.Dense(15, activation='relu', name="dense_layer_4")(dense_layer_3)

    output = layers.Dense(7, activation="softmax", name="output")(dense_layer_4)
    m = Model(inputs=input, outputs=output, name="23.07.21.jiwon.mlp.test")
    return m

def get_model(lr=0.0001, opt=Adam):
    model = get_layers()
    optimizer = opt(learning_rate=lr)
    loss = losses.CategoricalCrossentropy(from_logits=True) #(Jiwon): why from logits?

    #TODO: find a way other than in-place
    model.compile(
        optimizer = optimizer,
        loss = loss,
        #TODO: reagrding f1score, i need to render variable (not integer) referring last layer shape of model
        metrics = [metrics.Recall(), metrics.Precision(), F1Score(num_classes=7)]
        )
    return model

def fit_model(model, x, y):
    '''actual model fitting happends here, and do not have to return model since the model is reference'''
    params = {'epochs':20, 'bs':64}
    es = EarlyStopping(monitor="val_loss", mode="min", verbose=1, patience=5)

    model.fit(x=x,y=y, epochs=params['epochs'], batch_size=params['bs'], validation_split=0.2, callbacks=es)


def convert_one_hot(arry):
    '''get 1-d array and convert to 2-d one-hot encoding'''
    mat = np.zeros((arry.shape[0], arry.max()+1))
    mat[range(arry.shape[0]), arry] = 1
    return mat



model = get_model() #init model
y_train_2d = convert_one_hot(y_train) #convert y label to 2-d
fit_model(model, x_train, y_train_2d) #train

pred = model.predict(x=x_test, batch_size=64, verbose=1).argmax(-1) #predict and get maximum value's index

print(classification_report(y_test, pred, target_names=sorted(list(label_to_idx.keys())))) # report results