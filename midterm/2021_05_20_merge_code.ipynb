{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy_of_2021_05_12_merge_code.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zwxJN-alkO9V",
        "qAr2ogy3sLeW",
        "gRWgXS_wvonM",
        "oT7n4bqFvsa9",
        "Loh79wNZwS_o",
        "i4aEwhi9BjoL"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SpellOnYou/CLab21/blob/main/midterm/2021_05_20_merge_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwxJN-alkO9V"
      },
      "source": [
        "## Enviroment setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k232czaeCGy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09c8a95c-4103-4729-8333-58df6d6c273d"
      },
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "\n",
        "user = input('User name: ')\n",
        "password = getpass('Password: ')\n",
        "password = urllib.parse.quote(password) # your password is converted into url format\n",
        "# repo_name = input('Repo name: ')\n",
        "\n",
        "cmd_string = 'git clone https://{0}:{1}@github.com/{0}/CLab21.git'.format(user, password)\n",
        "\n",
        "os.system(cmd_string)\n",
        "cmd_string, password = \"\", \"\" # removing the password from the variable"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User name: SpellOnYou\n",
            "Password: ··········\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAr2ogy3sLeW"
      },
      "source": [
        "## Pre-processing(text representation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9HpXhzDALSn"
      },
      "source": [
        "Comment\n",
        "\n",
        "1. Generally don't need to use external library to read csv. open() function will work.\n",
        "2. Now we read one file twice(1. for sentence padding, 2. one-hot encoding) --> not efficient way to use computation resource :) should consider reconstructuring data loading process.\n",
        "3. Didn't investigate the whole algorithm. But BOW took much time compared to its data size. Need to be checked."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRWgXS_wvonM"
      },
      "source": [
        "### x data, tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woGbW17CslSY"
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "\n",
        "class PadMaxLength:\n",
        "\n",
        "    def __init__(self, file_name):\n",
        "        self.file = open(file_name)\n",
        "        self.csv_file = csv.DictReader(self.file)\n",
        "        self.list_padded_sentences = []\n",
        "        self.text = []\n",
        "        for col in self.csv_file:\n",
        "            self.text.append(col[\"text\"])\n",
        "\n",
        "    def min_max_sentences(self):\n",
        "        tokenized_sentences = []\n",
        "        # split each sentence into words\n",
        "        for sentence in self.text:\n",
        "            tokens = sentence.split()\n",
        "            tokenized_sentences.append(tokens)\n",
        "        # get longest sentence and its length\n",
        "        longest_sent = max(tokenized_sentences, key=len)\n",
        "        longest_sent_len = len(longest_sent)\n",
        "\n",
        "        # get shortest word and its length\n",
        "        shortest_sent = min(tokenized_sentences, key=len)\n",
        "        shortest_sent_len = len(shortest_sent)\n",
        "\n",
        "        return longest_sent_len, shortest_sent_len\n",
        "\n",
        "    def right_pad_sentences(self, max_sent_length):\n",
        "        max_len = round(max_sent_length * 0.50)  # Take 50% of the maximum sentence length to avoid sparsity\n",
        "        padded_sentences = []\n",
        "        # print(max_len)\n",
        "\n",
        "        for sentence in self.text:\n",
        "            sentence = sentence.strip()\n",
        "            sentence = sentence.split()\n",
        "\n",
        "            if len(sentence) > max_len:\n",
        "                a = sentence[:max_len]  # discard tokens longer than max_length\n",
        "                padded_sentences.append(a)\n",
        "\n",
        "            elif len(sentence) < max_len:\n",
        "                [sentence.append(\"0\") for i in\n",
        "                 range(max_len - len(sentence))]  # pad sentences with zeros smaller than max_length\n",
        "                padded_sentences.append(sentence)\n",
        "\n",
        "            else:\n",
        "                padded_sentences.append(sentence)\n",
        "\n",
        "        for pad_sent in padded_sentences:\n",
        "            list_sentences = ' '.join(pad_sent)\n",
        "            self.list_padded_sentences.append(list_sentences)\n",
        "\n",
        "        return self.list_padded_sentences\n",
        "\n",
        "    def merge_with(self, list2, list3):\n",
        "        merged = self.list_padded_sentences + list2 + list3\n",
        "        return merged\n",
        "\n",
        "\n",
        "class BagOfWords:\n",
        "\n",
        "    def __init__(self, all_padded_sentences, list_of_sentences):\n",
        "\n",
        "        # define punctuation and upper case alphabet\n",
        "        self.punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "        self.upper = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "        self.stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\n",
        "                          \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself',\n",
        "                          'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them',\n",
        "                          'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\",\n",
        "                          'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has',\n",
        "                          'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n",
        "                          'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
        "                          'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from',\n",
        "                          'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once',\n",
        "                          'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
        "                          'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n",
        "                          'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd',\n",
        "                          'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn',\n",
        "                          \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\",\n",
        "                          'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",\n",
        "                          'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won',\n",
        "                          \"won't\", 'wouldn', \"wouldn't\"]\n",
        "        self.vocab = self.generate_vocabulary(all_padded_sentences)  # Generate the vocabulary\n",
        "        # print(len(self.vocab))\n",
        "        self.dict_idx = self.indexing(self.vocab)  # Generate the indexing\n",
        "        self.word_count = self.count_dictionary(list_of_sentences)\n",
        "        self.N_sentences = len(list_of_sentences)\n",
        "\n",
        "    def lowercase_tokenize(self, padded_sentences):\n",
        "        lowercase = \"\"\n",
        "        for char in padded_sentences:\n",
        "            if char in self.upper:\n",
        "                k = ord(char)\n",
        "                l = k + 32\n",
        "                lowercase = lowercase + (chr(l))\n",
        "            elif char in self.punctuations:\n",
        "                continue\n",
        "            else:\n",
        "                lowercase = lowercase + char\n",
        "        lowercase = lowercase.strip()\n",
        "        tokenized = list(lowercase.split())\n",
        "        # print(tokenized)\n",
        "        return tokenized\n",
        "\n",
        "    def remove_stopwords(self, tokenized_sentences):\n",
        "        filtered_list = []\n",
        "        for token in tokenized_sentences:\n",
        "            if token in self.stopwords:\n",
        "                continue\n",
        "            else:\n",
        "                filtered_list.append(token)\n",
        "        return filtered_list\n",
        "\n",
        "    def generate_vocabulary(self, all_padded_sentences):\n",
        "        vocab = []\n",
        "        for sentence in all_padded_sentences:\n",
        "            tokenized_sentence = self.lowercase_tokenize(sentence)\n",
        "            filtered_tokenized_sentence = self.remove_stopwords(tokenized_sentence)\n",
        "            for word in filtered_tokenized_sentence:  # append only unique words\n",
        "                if word not in vocab:\n",
        "                    vocab.append(word)\n",
        "        return vocab\n",
        "\n",
        "    def indexing(self, tokens):\n",
        "        # Index dictionary to assign an index to each word in vocabulary\n",
        "        index_word = {}\n",
        "        i = 0\n",
        "        for word in tokens:\n",
        "            index_word[word] = i\n",
        "            i += 1\n",
        "        return index_word\n",
        "\n",
        "    def count_dictionary(self, input_sentences):\n",
        "        word_count = {}\n",
        "        for word in self.vocab:\n",
        "            word_count[word] = 0.0\n",
        "            for sentence in input_sentences:\n",
        "                if word in sentence:\n",
        "                    word_count[word] += 1.0\n",
        "        return word_count\n",
        "\n",
        "    # Term Frequency\n",
        "    def termfreq(self, sentence, word):\n",
        "        number_of_sentences = float(len(sentence))\n",
        "        occurrence = float(len([token for token in sentence if token == word]))\n",
        "        return occurrence / number_of_sentences\n",
        "\n",
        "    def inverse_doc_freq(self, word):\n",
        "        try:\n",
        "            word_occurrence = self.word_count[word] + 1.0\n",
        "        except KeyError:\n",
        "            word_occurrence = 1.0\n",
        "        return np.log(self.N_sentences / word_occurrence)\n",
        "\n",
        "    def tf_idf(self, input_sentences):\n",
        "        row = 0\n",
        "        tf_idf_vec = np.zeros((self.N_sentences, (len(self.vocab))))\n",
        "\n",
        "        for sentence in input_sentences:\n",
        "            tokenized_sentence = self.lowercase_tokenize(sentence)\n",
        "            filtered_tokenized_sentence = self.remove_stopwords(tokenized_sentence)\n",
        "            for word in filtered_tokenized_sentence:\n",
        "                tf = self.termfreq(sentence, word)\n",
        "                idf = self.inverse_doc_freq(word)\n",
        "\n",
        "                value = tf * idf\n",
        "                tf_idf_vec[row][self.dict_idx[word]] = value\n",
        "\n",
        "            row += 1\n",
        "\n",
        "        return tf_idf_vec"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXCuQpTO_f53"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oT7n4bqFvsa9"
      },
      "source": [
        "### y data, one-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDRL76A2vwK5"
      },
      "source": [
        "# from data_preprocessing.OneHotEncoding import *\n",
        "class OneHotEncoding:\n",
        "    def __init__(self, file_name):\n",
        "        self.mapping_dict = {}\n",
        "        #self.csv_file = pd.read_csv(file_name)\n",
        "        #self.labels = self.csv_file[\"label\"]\n",
        "\n",
        "        self.file = open(file_name)\n",
        "        self.csv_file = csv.DictReader(self.file)\n",
        "        self.labels = []\n",
        "        for col in self.csv_file:\n",
        "            self.labels.append(col[\"label\"])\n",
        "\n",
        "        self.target_labels = []\n",
        "        for word in self.labels:\n",
        "            if word not in self.target_labels:\n",
        "                self.target_labels.append(word)\n",
        "\n",
        "        #self.target_labels = self.labels.unique()\n",
        "        self.labels_dict = {}\n",
        "        self.mapping()\n",
        "\n",
        "    def get_unique_labels(self):\n",
        "\n",
        "        return self.target_labels\n",
        "\n",
        "    def mapping(self):\n",
        "        ### map each emotion to an integer\n",
        "        one_hot_encoded = []\n",
        "        for label_idx in range(len(self.target_labels)):\n",
        "            self.mapping_dict[self.target_labels[label_idx]] = label_idx\n",
        "        #print(self.mapping_dict)\n",
        "\n",
        "        for c in self.target_labels:\n",
        "            arr = list(np.zeros(len(self.target_labels), dtype=int))\n",
        "            arr[self.mapping_dict[c]] = 1\n",
        "            one_hot_encoded.append(arr)\n",
        "\n",
        "        self.generate_dictionary(one_hot_encoded)\n",
        "\n",
        "        return one_hot_encoded\n",
        "\n",
        "    def generate_dictionary(self, one_hot_encoded):\n",
        "        self.labels_dict = dict(zip(self.target_labels, one_hot_encoded))  # universal dict\n",
        "\n",
        "    def one_hot_encoding(self, encoded_dict=None):\n",
        "        df_labels = []\n",
        "        if encoded_dict is None:\n",
        "            encoded_dict = self.labels_dict\n",
        "        for c in self.labels:\n",
        "            if c in encoded_dict.keys():\n",
        "                df_labels.append(encoded_dict[c])\n",
        "        return np.array(df_labels)\n",
        "\n",
        "    def get_encoded_dict(self):\n",
        "        return self.labels_dict"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgy4Z0ye_tE2"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Loh79wNZwS_o"
      },
      "source": [
        "## Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6ckMvLZvF2Q"
      },
      "source": [
        "class Linear():\n",
        "    def __init__(self, w, b):\n",
        "        self.w, self.b = w, b\n",
        "    def forward(self, x): \n",
        "        self.inp = x\n",
        "        self.out = self.inp@self.w + self.b\n",
        "        return self.out\n",
        "    \n",
        "    def backward(self):\n",
        "        # set_trace()\n",
        "        self.inp.g = self.out.g @ self.w.t()\n",
        "        self.w.g = (self.inp.unsqueeze(-1) * self.out.g.unsqueeze(1)).sum(0)\n",
        "        self.b.g = self.out.g.sum(0)\n",
        "\n",
        "\n",
        "class Relu():\n",
        "    def forward(self, x):\n",
        "        self.inp = x\n",
        "        self.out = x.clamp_min(0.) - 0.5\n",
        "        return self.out\n",
        "\n",
        "    def backward(self):\n",
        "        self.inp.g = self.out.g* (self.inp>0).float()\n",
        "\n",
        "class CrossEntropy()"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1HzSYC27IhX",
        "outputId": "7ddc31e9-02e9-4d2c-a49d-6e6ba78b1ab4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -q ipdb\n",
        "from ipdb import set_trace"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 788kB 7.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 31.3MB/s \n",
            "\u001b[?25h  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: jupyter-console 5.2.0 has requirement prompt-toolkit<2.0.0,>=1.0.0, but you'll have prompt-toolkit 3.0.18 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipython~=5.5.0, but you'll have ipython 7.23.1 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H584_Yc07IL6"
      },
      "source": [
        "class CrossEntropy():\n",
        "    def __call__(self, pred, y):\n",
        "        \n",
        "        self.yhat, self.y = pred, y\n",
        "        #P(\\hat{y})\n",
        "        self.log_p_yhat = self.log_softmax(pred)\n",
        "        self.out = self.nll(self.log_p_yhat, y)\n",
        "        \n",
        "        return self.out\n",
        "\n",
        "    #negative log likelihood\n",
        "    def nll(self, pred, y):\n",
        "        # print(pred.shape, y.shape)\n",
        "        return -pred[range(y.shape[0]), y.max(-1).indices].mean()\n",
        "\n",
        "    def log_softmax(self, x): return x - x.exp().sum(-1,keepdim=True).log()\n",
        "\n",
        "    def backward(self, inp):\n",
        "        # set_trace()\n",
        "        self.yhat.g = (inp.unsqueeze(1)*(self.yhat - self.y).unsqueeze(-1)).sum(-1)\n",
        "        # self.yhat.g = (self.yhat.exp()/(self.yhat.exp().sum(-1,keepdim=True))) - self.y\n",
        "\n",
        "class DummyModel():\n",
        "    def __init__(self, w1, b1, w2, b2):\n",
        "        self.loss = CrossEntropy()\n",
        "        self.layers = [Linear(w1,b1), Relu(), Linear(w2, b2)]\n",
        "    \n",
        "    def forward(self, x):\n",
        "        self.x= x\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        self.out = x\n",
        "        return self.out\n",
        "\n",
        "    def backward(self, x2):\n",
        "        self.loss.backward(x2)\n",
        "        for layer in reversed(self.layers):\n",
        "            layer.backward()"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4aEwhi9BjoL"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qdw_z2jsh02",
        "outputId": "05e369e4-ab97-453b-9c60-d9c3ffe66e50"
      },
      "source": [
        "%cd /content/CLab21/"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/CLab21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "EWlD51R_68gY"
      },
      "source": [
        "# # in case you need debug\n",
        "# !pip install -q ipdb\n",
        "# from ipdb import set_trace"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqfMvzIJ--Qz"
      },
      "source": [
        "import numpy as np\n",
        "import csv, math\n",
        "from torch import tensor, float32, randn, no_grad"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vi-LbQsQdngW"
      },
      "source": [
        "from pathlib import Path\n",
        "rpath = Path('/content/CLab21/data/emotions')"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5Qh2Pe1sdFz"
      },
      "source": [
        "train_file = rpath/\"isear/isear-train-modified.csv\"\n",
        "val_file = rpath/\"isear/isear-val-modified.csv\"\n",
        "test_file = rpath/\"isear/isear-test-modified.csv\""
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFXXMSNILzWK"
      },
      "source": [
        "- padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfnFBqGSLU7P"
      },
      "source": [
        "# %%timeit\n",
        "pml_train = PadMaxLength(train_file)\n",
        "pml_val = PadMaxLength(val_file)\n",
        "pml_test = PadMaxLength(test_file)\n",
        "\n",
        "max_sent, min_sent = pml_train.min_max_sentences()\n",
        "\n",
        "sentences_padded_train = pml_train.right_pad_sentences(max_sent)\n",
        "sentences_padded_val = pml_val.right_pad_sentences(max_sent)\n",
        "sentences_padded_test = pml_test.right_pad_sentences(max_sent)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYkbLH5cL2x9"
      },
      "source": [
        "- vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBy365HgL1E4"
      },
      "source": [
        "# %timeit\n",
        "vocab_list = pml_train.merge_with(sentences_padded_val, sentences_padded_test)  # Vocab over all files\n",
        "\n",
        "bow_train = BagOfWords(vocab_list, sentences_padded_train)  # Sentences to create the vocabulary\n",
        "bow_val = BagOfWords(vocab_list, sentences_padded_val)\n",
        "bow_test = BagOfWords(vocab_list, sentences_padded_test)\n",
        "\n",
        "tf_idf_train = bow_train.tf_idf(sentences_padded_train)\n",
        "tf_idf_val = bow_val.tf_idf(sentences_padded_val)\n",
        "tf_idf_test = bow_test.tf_idf(sentences_padded_test)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0PzRMeQMF_9"
      },
      "source": [
        "- y data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eXy6viABoQI"
      },
      "source": [
        "ohe_train = OneHotEncoding(train_file)\n",
        "ohe_val = OneHotEncoding(val_file)\n",
        "ohe_test = OneHotEncoding(test_file)\n",
        "\n",
        "y_train = ohe_train.one_hot_encoding()\n",
        "reference_dict = ohe_train.get_encoded_dict()\n",
        "\n",
        "y_val = ohe_val.one_hot_encoding(reference_dict)\n",
        "y_test = ohe_test.one_hot_encoding(reference_dict)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeZelBjOMJke"
      },
      "source": [
        "- torch tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE8tNCSTwkfm"
      },
      "source": [
        "train_x, train_y = tensor(tf_idf_train, dtype=float32), tensor(y_train)\n",
        "valid_x, valid_y = tensor(tf_idf_val, dtype=float32), tensor(y_val)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MU_2O7HYnsG9",
        "outputId": "2425100f-9485-42e0-ccad-618b544a4016"
      },
      "source": [
        "train_x.shape, valid_x.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([5344, 9226]), torch.Size([1148, 9226]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRwKN6JhCdEh"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCGjqTKMohOR"
      },
      "source": [
        "train_x = train_x[:, :3000]\n",
        "valid_x = valid_x[:, :3000]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnZBx1tYwcke"
      },
      "source": [
        "#n: data size m: n_text_feature h: hidden node c: out node\n",
        "n, m, h, c = *train_x.shape, 100, train_y.shape[1]"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqSePVr9yNTq"
      },
      "source": [
        "w1 = randn(m, h) / math.sqrt(h)\n",
        "w2 = randn(h, c)\n",
        "b1 = randn(h)\n",
        "b2 = randn(c)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjpCwynUydkR"
      },
      "source": [
        ""
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZZVUjYc8187",
        "collapsed": true
      },
      "source": [
        "# model.forward(train_x[:32], train_y[:32])"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ76bZPRwZcT"
      },
      "source": [
        "model = DummyModel(w1, b1, w2, b2)\n",
        "\n",
        "def train(epochs, bs, lr):\n",
        "    for e in range(epochs):\n",
        "        for bs_i in range((n-1)//bs + 1):\n",
        "            tot_w_mean, tot_w_std = 0, 0\n",
        "            str_idx, end_idx = bs_i*bs, (bs_i+1)*bs\n",
        "            x_batch, y_batch = train_x[str_idx:end_idx], train_y[str_idx:end_idx]\n",
        "            prediction = model.forward(x_batch)\n",
        "            loss = model.loss(prediction, y_batch)\n",
        "\n",
        "            model.backward(model.layers[-1].inp)\n",
        "            \n",
        "            with no_grad():\n",
        "                for layer in model.layers:\n",
        "                    if hasattr(layer, 'w'): #if they have parameter attribute\n",
        "                        tot_w_mean+= layer.w.g.mean()\n",
        "                        tot_w_std += layer.w.g.std()\n",
        "                        layer.w -= layer.w.g * lr\n",
        "                        layer.b -= layer.b.g * lr\n",
        "                        layer.w.g.zero_() #initialize them to zero\n",
        "                        layer.b.g.zero_()\n",
        "            if bs_i % 20 ==0: print(tot_w_mean/bs, tot_w_std/bs)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpPXcyAuyf5g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34787e0b-7e8e-4d20-e24a-8a86ad906b51"
      },
      "source": [
        "train(1, 32, 1e-05)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-1.0217) tensor(32.5078)\n",
            "tensor(-2.8342e-08) tensor(0.0037)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-psGH5eN6Vg0"
      },
      "source": [
        "# loss after training\n",
        "pred = model.forward(train_x[:32])\n",
        "loss = model.loss(pred, train_y[:32])"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJgMxoAS8RFE",
        "outputId": "d2c1cc2e-2927-4606-adfe-d7aeddc5c699",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "loss"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(35.0006)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGakSGQqDc4p"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye023wz2fv0Q"
      },
      "source": [
        "pred_valid = model.forward(valid_x)\n",
        "loss_valid = model.loss(pred_valid, valid_y)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u2oi54P8cIz",
        "outputId": "b8e36e2f-a3ca-4027-bc17-3c65b7b7e2a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "loss_valid"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(33.7810)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4awxiCLpgs0"
      },
      "source": [
        "softmax_pred = model.loss.log_softmax(model.loss.yhat)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAhIVrL18kWe",
        "outputId": "347132f8-408c-4b2d-a022-3a97b76938b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "softmax_pred"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-16.1844, -35.9802, -33.1690,  ..., -75.9095, -52.7449, -21.0293],\n",
              "        [-16.1849, -35.9800, -33.1700,  ..., -75.9098, -52.7455, -21.0298],\n",
              "        [-16.1854, -35.9798, -33.1709,  ..., -75.9101, -52.7460, -21.0303],\n",
              "        ...,\n",
              "        [-16.1863, -35.9794, -33.1725,  ..., -75.9106, -52.7469, -21.0311],\n",
              "        [-16.1849, -35.9800, -33.1698,  ..., -75.9097, -52.7454, -21.0297],\n",
              "        [-16.1841, -35.9803, -33.1683,  ..., -75.9093, -52.7446, -21.0290]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk6O6M2YDeE_"
      },
      "source": [
        "class Fscore():\n",
        "    def __init__(self, inp, trg):\n",
        "        self.inp, self.trg = inp.max(-1).indices, trg.max(-1).indices\n",
        "        self.c = inp.shape[1]\n",
        "    def __call__(self, alpha = 0.5):\n",
        "        self.precision()\n",
        "        self.recall()\n",
        "        f1 = map(\n",
        "            self.fscore, self.tot_pre, self.tot_rec\n",
        "            )\n",
        "        return self.tot_pre, self.tot_rec, list(f1)\n",
        "    def fscore(self, x, y):\n",
        "        return (2*x*y)/(x+y)\n",
        "    \n",
        "    def precision(self):\n",
        "        self.tot_pre= []\n",
        "        for i in range(self.c):\n",
        "            numer = self.inp == self.trg\n",
        "            denom = self.inp ==i\n",
        "            if not sum(denom)==0: self.tot_pre += [sum(numer) / sum(denom)]\n",
        "            else: self.tot_pre += [0.]\n",
        "\n",
        "    def recall(self):\n",
        "        self.tot_rec= []\n",
        "        for i in range(self.c):\n",
        "            numer = self.inp == self.trg\n",
        "            denom = self.trg ==i\n",
        "            if not sum(denom)==0: self.tot_rec += [sum(numer) / sum(denom)]\n",
        "            else: self.tot_rec += [0.]"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJUJxDaasgRu"
      },
      "source": [
        "measure = Fscore(softmax_pred, valid_y)\n",
        "p, r, f = measure()"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8fdhJEldm30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9178e465-a27e-4891-ec7e-016290406a46"
      },
      "source": [
        "p, r, f"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0.0, 0.0, 0.0, tensor(0.1385), 0.0, 0.0, 0.0],\n",
              " [tensor(1.0530),\n",
              "  tensor(0.9298),\n",
              "  tensor(0.9521),\n",
              "  tensor(1.),\n",
              "  tensor(0.9636),\n",
              "  tensor(0.9937),\n",
              "  tensor(0.9086)],\n",
              " [tensor(0.),\n",
              "  tensor(0.),\n",
              "  tensor(0.),\n",
              "  tensor(0.2433),\n",
              "  tensor(0.),\n",
              "  tensor(0.),\n",
              "  tensor(0.)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovqVJsTsbQlW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRaBDwrVmNlT"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k8n2Iw4mOfb"
      },
      "source": [
        "## [sklearn.metrics.classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HxFGnf5mYUH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18b4e4d8-dac4-4706-be2a-c639f96ec80d"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "trg_names = list(reference_dict.keys())\n",
        "print(classification_report(y_true = measure.trg, y_pred = measure.inp, target_names=trg_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         joy       0.00      0.00      0.00       151\n",
            "        fear       0.00      0.00      0.00       171\n",
            "       shame       0.15      1.00      0.25       167\n",
            "     disgust       0.00      0.00      0.00       159\n",
            "       guilt       0.00      0.00      0.00       165\n",
            "       anger       0.00      0.00      0.00       160\n",
            "     sadness       0.00      0.00      0.00       175\n",
            "\n",
            "    accuracy                           0.15      1148\n",
            "   macro avg       0.02      0.14      0.04      1148\n",
            "weighted avg       0.02      0.15      0.04      1148\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}